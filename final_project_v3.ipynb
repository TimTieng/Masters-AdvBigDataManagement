{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2406e59f-5170-40bf-91e9-280d28d3913c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project Plan: Wikipedia Clickstream Analysis\n",
    "\n",
    "## Topic: Wikipedia Clickstream Analysis\n",
    "\n",
    "### Data Source: Wikimedia Clickstream Dumps\n",
    "\n",
    "**Data Format**: TSV (Tab-Separated Values)\n",
    "\n",
    "**Data Description**:\n",
    "The dataset contains information about user navigation paths on Wikipedia. \n",
    "\n",
    "It records the source and target articles and the number of times users navigated from one article to another. \n",
    "\n",
    "For the analysis, we will use the clickstream data for May 2024, which includes navigation paths for various language editions of Wikipedia.\n",
    "\n",
    "\n",
    "### Data Preparation and Analysis Plan:\n",
    "\n",
    "#### Part I: Setup and Data Acquisition\n",
    "\n",
    "1. **Setup Environment**:\n",
    "    - Configure Docker containers for Minio, Hadoop (HDFS), and MongoDB.\n",
    "    - Install necessary Python libraries: PySpark, pandas, matplotlib, etc.\n",
    "\n",
    "2. **Download Data**:  \n",
    "    - Use Minio client to download clickstream data for all months of 2024 from Wikimedia's clickstream dumps.\n",
    "    \n",
    "    \n",
    "    ```bash\n",
    "    wget https://dumps.wikimedia.org/other/clickstream/2024-01/clickstream-enwiki-2024-01.tsv.gz\n",
    "    wget https://dumps.wikimedia.org/other/clickstream/2024-02/clickstream-enwiki-2024-02.tsv.gz\n",
    "    wget https://dumps.wikimedia.org/other/clickstream/2024-03/clickstream-enwiki-2024-03.tsv.gz\n",
    "    wget https://dumps.wikimedia.org/other/clickstream/2024-04/clickstream-enwiki-2024-04.tsv.gz\n",
    "    wget https://dumps.wikimedia.org/other/clickstream/2024-05/clickstream-enwiki-2024-05.tsv.gz\n",
    "    ```\n",
    "    \n",
    "    \n",
    "    For now, the example size of the May 2024 file are approximately:\n",
    "    \n",
    "    \n",
    "    - January: 514.75 MB\n",
    "    - February: 514.75 MB\n",
    "    - March: 514.75 MB\n",
    "    - April: 514.75 MB\n",
    "    - May: 514.75 MB\n",
    "    \n",
    "    \n",
    "    **Total Estimated Size**: ≈ 2.51 GB\n",
    "\n",
    "\n",
    "3. **Store Data in HDFS**:\n",
    "    - Upload the downloaded data to HDFS for efficient distributed storage.\n",
    "\n",
    "#### Part II: Data Processing and Loading\n",
    "\n",
    "1. **Preprocess Data**:\n",
    "    - Load data from HDFS into PySpark DataFrames.\n",
    "    - Perform necessary data cleaning and transformations.\n",
    "\n",
    "\n",
    "#### Part III: Data Analysis\n",
    "\n",
    "1. **Load Data from MongoDB**:\n",
    "    - Load the preprocessed data from MongoDB into a Spark DataFrame for analysis.\n",
    "\n",
    "2. **Apply KNN Clustering**:\n",
    "    - Use PySpark's MLlib to perform KNN clustering.\n",
    "\n",
    "3. **Analysis**:\n",
    "    - Identify article relationships and analyze click patterns.\n",
    "    - Discover clusters of related articles using KNN clustering.\n",
    "    - Provide insights into content organization and recommendations.\n",
    "\n",
    "#### Part IV: Visualization\n",
    "\n",
    "1. **Create Visualizations**:\n",
    "    - Use visualization libraries like Matplotlib to create interactive graphs presenting the findings.\n",
    "\n",
    "#### Big Limitation: Due to lack of resources to each VM, datasets where reduced for the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5367bf99-54b6-4ca9-81a9-e4fb0fa8aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.34.122-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.35.0,>=1.34.122\n",
      "  Downloading botocore-1.34.122-py3-none-any.whl (12.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 426 kB/s eta 0:00:01    |██                              | 747 kB 1.6 MB/s eta 0:00:08��██████████                  | 5.4 MB 1.6 MB/s eta 0:00:05    | 6.9 MB 3.1 MB/s eta 0:00:02     |█████████████████████▍          | 8.2 MB 3.1 MB/s eta 0:00:02�████████████████▋     | 10.2 MB 3.1 MB/s eta 0:00:01     |███████████████████████████▉    | 10.7 MB 3.1 MB/s eta 0:00:01     |███████████████████████████████ | 11.9 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 4.0 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.122->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.122->boto3) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.122->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.122 botocore-1.34.122 jmespath-1.0.1 s3transfer-0.10.1\n",
      "Collecting pymango\n",
      "  Downloading pymango-0.1.1.tar.gz (2.7 kB)\n",
      "Requirement already satisfied: requests>=2.4.3 in /opt/conda/lib/python3.9/site-packages (from pymango) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.4.3->pymango) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.4.3->pymango) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.4.3->pymango) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.4.3->pymango) (1.26.6)\n",
      "Building wheels for collected packages: pymango\n",
      "  Building wheel for pymango (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pymango: filename=pymango-0.1.1-py3-none-any.whl size=4053 sha256=9a7234a986e1328786d457c436d2f54ab6469ba46906b3764cb37004cbe88c60\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/c8/80/74/5d2526b312dc158c4fc14728827509a048e2991a90dafed08b\n",
      "Successfully built pymango\n",
      "Installing collected packages: pymango\n",
      "Successfully installed pymango-0.1.1\n"
     ]
    }
   ],
   "source": [
    "# Package Installations:\n",
    "!pip install boto3\n",
    "!pip install pymango\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55d679-6e8a-48b9-8fba-d46b8e56e6ed",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Acquisition\n",
    "\n",
    "To start this project, our group devided on using minio and Hadoop Distributed File System (HDFS) to meet the scope of the project.\n",
    "\n",
    "### MinIO Overview & Benefits:\n",
    "- Minio is an open-source object storage server that is compatible with Amazon S3 APIs. It is designed to store unstructured data such as photos, videos, log files, backups\n",
    "- Minio enables users to set up their own cloud storage solution that behaves like Amazon S3 but on their own servers or private cloud environment. It can be used as a standalone application or deployed into existing infrastructure using orchestration tools like Kubernetes. \n",
    "\n",
    "- Benefit 1- Scalability: It can scale out horizontally, allowing it to handle petabytes of data and millions of objects efficiently. It supports distributed mode which helps in scaling and provides redundancy and high availability.\n",
    "- Benefit 2 - Simplicity: Minio's simplicity allows it to be easily deployed in various environments.\n",
    "- Benefit 3 - High Performance: Minio is designed for high performance with its minimalist approach to storage. It is optimized for workloads that require high-speed access to large volumes of data.\n",
    "\n",
    "### HDFS Overview & Benefits:\n",
    "- Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It is a key component of Apache Hadoop, which is an ecosystem of open source components that fundamentally changes the way enterprises store, process, and analyze data. \n",
    "- HDFS stores large files across multiple machines in a large cluster. It breaks down large files into smaller blocks, and distributes them across the nodes in the cluster. This distribution enables reliable, extremely rapid computations and uses a master/slave architecture where a single master node (NameNode) manages the file system metadata and several slave nodes (DataNodes) store the actual data.\n",
    "\n",
    "- Benefit 1 - Scalability: HDFS can scale to accommodate thousands of nodes and millions of files. It's designed to work with petabyte-scale data stores, allowing enterprises to expand their data analysis capabilities easily.\n",
    "- Benefit 2 - Fault Tolerance: By storing multiple copies of data blocks at different nodes (replication), HDFS ensures that the system is fault-tolerant. If a node fails, data can be retrieved from another node that has a copy of the same data block, ensuring data availability and continuity of operations.\n",
    "- Benefit 3 - Data Integrity: HDFS checks data integrity by using checksums. When data is written to HDFS, checksums are calculated and verified. If a corrupted block is detected, HDFS automatically replaces it with one of the replicated blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45df5284-17c2-47fd-99db-e67a6116cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0121e079-f075-4a02-b42b-73cf85bcbf8c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (179ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.375!aws-java-sdk-bundle.jar (19900ms)\n",
      ":: resolution report :: resolve 5855ms :: artifacts dl 20098ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0121e079-f075-4a02-b42b-73cf85bcbf8c\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (96887kB/289ms)\n",
      "24/06/10 12:59:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket clickstream-data created.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import requests\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, EndpointConnectionError\n",
    "\n",
    "# Initialize Spark session with the provided configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinioSparkSession\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create Minio client\n",
    "minio_client = boto3.client('s3',\n",
    "                            endpoint_url='http://minio:9000',\n",
    "                            aws_access_key_id='minio',\n",
    "                            aws_secret_access_key='SU2orange!')\n",
    "\n",
    "# Create a bucket\n",
    "bucket_name = 'clickstream-data'\n",
    "try:\n",
    "    minio_client.create_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} created.')\n",
    "except minio_client.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print(f'Bucket {bucket_name} already exists.')\n",
    "except NoCredentialsError:\n",
    "    print(\"Credentials not available\")\n",
    "except PartialCredentialsError:\n",
    "    print(\"Incomplete credentials provided\")\n",
    "except EndpointConnectionError:\n",
    "    print(\"Could not connect to the endpoint URL\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13785998",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "This code block serves multiple purposes, primarily involving setting up and interacting with a Spark session configured to work with a Minio S3 storage, and handling connections and operations with that storage using the boto3 library.\n",
    "\n",
    "Code Purpose:\n",
    "\n",
    "1. Initialize a Spark Session: This is a foundational step when working with distributed data processing with Spark. This provides a way to leverage Spark functionality. \n",
    "2. Create Minio Client: The Minio Client allows us to provide access to the Minio API. Furthermore, this allows for simple storage operations in conjunction with AWS S3. Lastly, this allows us to integrate with other systems, like HDFS.\n",
    "3. Create a Minio Bucket: This is essential for organizing and managing our wikipedia clickstream data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030eafce-3589-4c36-86f8-f792870fd4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded /tmp/2024-02.tsv.gz to clickstream-data/2024-02.tsv.gz\n",
      "Uploaded /tmp/2024-03.tsv.gz to clickstream-data/2024-03.tsv.gz\n",
      "Uploaded /tmp/2024-04.tsv.gz to clickstream-data/2024-04.tsv.gz\n",
      "Uploaded /tmp/2024-05.tsv.gz to clickstream-data/2024-05.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "# URLs for the clickstream files for 2024\n",
    "clickstream_urls = {\n",
    "     #\"2024-01\": \"https://dumps.wikimedia.org/other/clickstream/2024-01/clickstream-enwiki-2024-01.tsv.gz\",\n",
    "     \"2024-02\": \"https://dumps.wikimedia.org/other/clickstream/2024-02/clickstream-enwiki-2024-02.tsv.gz\",\n",
    "     \"2024-03\": \"https://dumps.wikimedia.org/other/clickstream/2024-03/clickstream-enwiki-2024-03.tsv.gz\",\n",
    "     \"2024-04\": \"https://dumps.wikimedia.org/other/clickstream/2024-04/clickstream-enwiki-2024-04.tsv.gz\",\n",
    "     \"2024-05\": \"https://dumps.wikimedia.org/other/clickstream/2024-05/clickstream-enwiki-2024-05.tsv.gz\"\n",
    "}\n",
    "\n",
    "# Download and upload files to Minio\n",
    "for key, url in clickstream_urls.items():\n",
    "    response = requests.get(url)\n",
    "    file_path = f'/tmp/{key}.tsv.gz'\n",
    "    \n",
    "    # Save the file locally\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Upload the file to Minio\n",
    "    try:\n",
    "        minio_client.upload_file(file_path, bucket_name, f'{key}.tsv.gz')\n",
    "        print(f'Uploaded {file_path} to {bucket_name}/{key}.tsv.gz')\n",
    "    except EndpointConnectionError:\n",
    "        print(f\"Could not connect to the endpoint URL while uploading {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while uploading {file_path}: {e}\")\n",
    "\n",
    "# Verify uploaded files\n",
    "# try:\n",
    "#     response = minio_client.list_objects_v2(Bucket=bucket_name)\n",
    "#     for obj in response.get('Contents', []):\n",
    "#         print(obj['Key'])\n",
    "# except EndpointConnectionError:\n",
    "#     print(\"Could not connect to the endpoint URL while listing objects\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred while listing objects: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746a581",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "This script automates the process of downloading clickstream data files from Wikimedia and uploading them to a Minio bucket, facilitating data storage and management in a cloud environment.\n",
    "\n",
    "We store our clickstream urls into a self-defined dictionary called, \"clickstream_urls\" that we will iterate through to store the data into our minio environment. Due to environment constraints, we decided to use only 1 URL to load data, (2024-01 clickstream data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db9ac8-5366-46e0-aed5-9637e36ebe72",
   "metadata": {},
   "source": [
    "## Part 2: Data Processing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2fc303-5b39-499e-b058-225da0a8164a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01.tsv.gz\n",
      "2024-02.tsv.gz\n",
      "2024-03.tsv.gz\n",
      "2024-04.tsv.gz\n",
      "2024-05.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended data from s3a://clickstream-data/2024-01.tsv.gz\n",
      "Appended data from s3a://clickstream-data/2024-02.tsv.gz\n",
      "Appended data from s3a://clickstream-data/2024-03.tsv.gz\n",
      "Appended data from s3a://clickstream-data/2024-04.tsv.gz\n",
      "Appended data from s3a://clickstream-data/2024-05.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data written to hdfs://namenode:8020/clickstream-data/combined_clickstream.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, EndpointConnectionError\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session for Minio to HDFS migration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinioToHDFSMigration\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:8020\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create Minio client\n",
    "minio_client = boto3.client('s3',\n",
    "                            endpoint_url='http://minio:9000',\n",
    "                            aws_access_key_id='minio',\n",
    "                            \n",
    "                            aws_secret_access_key='SU2orange!')\n",
    "\n",
    "# Verify uploaded files in Minio\n",
    "bucket_name = 'clickstream-data'\n",
    "file_keys = []\n",
    "try:\n",
    "    response = minio_client.list_objects_v2(Bucket=bucket_name)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            file_keys.append(obj['Key'])\n",
    "            print(obj['Key'])\n",
    "    else:\n",
    "        print(f\"No files found in bucket {bucket_name}.\")\n",
    "except NoCredentialsError:\n",
    "    print(\"Credentials not available\")\n",
    "except PartialCredentialsError:\n",
    "    print(\"Incomplete credentials provided\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while listing objects: {e}\")\n",
    "\n",
    "# Read files from Minio and append data into a single DataFrame\n",
    "combined_df = None\n",
    "\n",
    "for key in file_keys:\n",
    "    file_path = f's3a://{bucket_name}/{key}'\n",
    "    try:\n",
    "        df = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"false\").csv(file_path)\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = combined_df.union(df)\n",
    "        print(f'Appended data from {file_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "# Write the combined DataFrame to HDFS\n",
    "hdfs_path = 'hdfs://namenode:8020/clickstream-data/combined_clickstream.parquet'\n",
    "if combined_df is not None:\n",
    "    try:\n",
    "        combined_df.write.mode('overwrite').parquet(hdfs_path)\n",
    "        print(f'Combined data written to {hdfs_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while writing to HDFS: {e}\")\n",
    "else:\n",
    "    print(\"No data to write to HDFS\")\n",
    "\n",
    "# # Verify combined data in HDFS\n",
    "# try:\n",
    "#     hdfs_combined_df = spark.read.parquet(hdfs_path)\n",
    "#     hdfs_combined_df.show(10)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred while reading from HDFS: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce95281",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "The purpose of this code block is to conduct data migration from Minio to HDFS. There are essentially 5 core operations being executed within this code block:\n",
    "\n",
    "1. Initialize a a new Spark Session\n",
    "2. Create a new Minio Client using boto3 package\n",
    "3. Identify files in Minio Bucket\n",
    "4. Read and Combine Data\n",
    "5. Write Combined Data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88510ad-e411-4975-a465-a4426b144681",
   "metadata": {},
   "source": [
    "## Part 3: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6ba904-6b7d-48e7-a4f2-8bee5235c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:====================================================>   (26 + 2) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9999985610039243\n",
      "Cluster Centers: \n",
      "[205.17293681]\n",
      "[1.18475219e+08]\n",
      "[4119056.72727273]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session for running K-means\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HDFSKMeansClustering\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:8020\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Path to the combined data in HDFS\n",
    "hdfs_path = 'hdfs://namenode:8020/clickstream-data/combined_clickstream.parquet'\n",
    "\n",
    "# Read the combined data from HDFS\n",
    "df = spark.read.parquet(hdfs_path)\n",
    "\n",
    "# Sample 15% of the data\n",
    "df = df.sample(withReplacement=False, fraction=0.15, seed=42)\n",
    "\n",
    "# Rename columns\n",
    "df = df.withColumnRenamed(\"_c0\", \"source\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"target\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"link_type\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"clicks\")\n",
    "\n",
    "# Convert necessary columns to numerical types if needed\n",
    "df = df.withColumn(\"clicks\", col(\"clicks\").cast(\"integer\"))\n",
    "\n",
    "# Drop rows with null values in the clicks column\n",
    "df = df.na.drop(subset=[\"clicks\"])\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"clicks\"],  # We only use clicks for clustering as an example\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"  # Skip rows with invalid (null) values\n",
    ")\n",
    "\n",
    "# Transform the data\n",
    "assembled_df = assembler.transform(df)\n",
    "\n",
    "# Run K-means\n",
    "kmeans = KMeans().setK(3).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(assembled_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(assembled_df)\n",
    "\n",
    "# Evaluate clustering\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette with squared euclidean distance = {silhouette}\")\n",
    "\n",
    "# Show the result\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e9fcf-fb63-476a-bb14-52bfc46563ce",
   "metadata": {},
   "source": [
    "## Part 4: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f838c2e-d280-41db-afcc-fa352f4d9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"IPC Client (134490591) connection to namenode/10.10.10.10:8020 from jovyan\" java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o493.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/1221346755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clicks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o493.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "# Collect results for visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "results = predictions.select(\"clicks\", \"prediction\").toPandas()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(results[\"clicks\"], results[\"prediction\"], c=results[\"prediction\"], cmap=\"viridis\", marker='o')\n",
    "plt.xlabel(\"Clicks\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.title(\"K-means Clustering of Clickstream Data\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1413e-4817-4583-b461-73eedca41b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
